{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Peer Review Data for the ICLR Conference from OpenReview\n",
    "- This notebook creates a web crawler to extract all peer review data for the ICLR conference from OpenReview.\n",
    "- The data formatted as a CSV file, then saved to data/raw_peer_review_data.csv.\n",
    "- The rest of the cleaning, which is different for each year of the conference, is done in cleaning_peer_review_data.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "def get_page_source(\n",
    "    driver, wait_condition=(By.CSS_SELECTOR, \"div.forum-container div.note_contents\")\n",
    "):\n",
    "    \"\"\"\n",
    "    Waits for the presence of a specific element on the page and returns the page source as a BeautifulSoup object.\n",
    "\n",
    "    Args:\n",
    "        driver: Selenium WebDriver instance.\n",
    "        wait_condition: Tuple containing the locator strategy and locator value for the element to wait for.\n",
    "\n",
    "    Returns:\n",
    "        BeautifulSoup object representing the page source.\n",
    "    \"\"\"\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located(wait_condition))\n",
    "    html_content = driver.page_source\n",
    "    soup = BeautifulSoup(html_content, \"lxml\")\n",
    "    return soup\n",
    "\n",
    "\n",
    "def get_paper_data(page_soup, paper_id):\n",
    "    \"\"\"\n",
    "    Extracts paper data from the page source.\n",
    "\n",
    "    Args:\n",
    "        page_soup: BeautifulSoup object representing the page source.\n",
    "        paper_id: ID of the paper.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing the extracted paper data.\n",
    "    \"\"\"\n",
    "    soup = page_soup.find(\"div\", class_=\"forum-container\").find(\"div\", class_=\"note\")\n",
    "    paper_name = soup.find(\"h2\", class_=\"note_content_title\").text.strip()\n",
    "\n",
    "    # Extract authors and their emails\n",
    "    author_data = {}\n",
    "    try:\n",
    "        authors = soup.find_all(\"a\", class_=\"profile-link\")\n",
    "        for i, author in enumerate(authors, start=1):\n",
    "            author_name = author.text.strip()\n",
    "            author_link = author[\"href\"].split(\"=\")[1]\n",
    "            author_data[f\"Author {i} name\"] = author_name\n",
    "            author_data[f\"Author {i} id\"] = author_link\n",
    "    except:\n",
    "        print(\"Authors not found for paper\", paper_id)\n",
    "\n",
    "    # Extract date published and date modified\n",
    "    date_published, date_modified = None, None\n",
    "    try:\n",
    "        date_published, date_modified = re.findall(\n",
    "            r\"Published: (.*?), Last Modified: (.*)\",\n",
    "            soup.find(\"span\", class_=\"date\").text,\n",
    "        )[0]\n",
    "    except:\n",
    "        try:\n",
    "            date_text = soup.find(\"span\", class_=\"date\").text\n",
    "            date_published, date_modified = re.findall(\n",
    "                r\"(\\d{2} \\w{3} \\d{4})\", date_text\n",
    "            )\n",
    "        except:\n",
    "            print(\"Date not found for paper\", paper_id)\n",
    "\n",
    "    # Extract type of paper\n",
    "    type_of_paper = None\n",
    "    try:\n",
    "        type_of_paper = (\n",
    "            soup.find(\"span\", class_=\"date\").find_next_sibling(\"span\").text.strip()\n",
    "        )\n",
    "    except:\n",
    "        print(\"Type of paper not found for paper\", paper_id)\n",
    "\n",
    "    # Extract keywords and other information from note contents\n",
    "    note_contents = soup.find_all(\"div\", class_=\"note_contents\")\n",
    "    additional_info = {}\n",
    "    for note in note_contents:\n",
    "        try:\n",
    "            key = note.find(\"span\", class_=\"note_content_field\").text.strip(\": \")\n",
    "            value = note.find(\"span\", class_=\"note_content_value\").text.strip()\n",
    "            additional_info[key] = value\n",
    "        except:\n",
    "            print(\"Additional info not found for paper\", paper_id)\n",
    "\n",
    "    paper_info = {\n",
    "        \"Paper ID\": paper_id,\n",
    "        \"Paper name\": paper_name,\n",
    "        **author_data,\n",
    "        \"Date published\": date_published,\n",
    "        \"Date last modified\": date_modified,\n",
    "        \"Type of paper\": type_of_paper,\n",
    "        **additional_info,\n",
    "    }\n",
    "\n",
    "    return paper_info\n",
    "\n",
    "\n",
    "def get_comment_data(page_soup, driver, paper_id, in_subcomments=False):\n",
    "    \"\"\"\n",
    "    Extracts comment data from the page source.\n",
    "\n",
    "    Args:\n",
    "        page_soup: BeautifulSoup object representing the page source.\n",
    "        driver: Selenium WebDriver instance.\n",
    "        paper_id: ID of the paper.\n",
    "        in_subcomments: Boolean indicating whether the function is currently extracting subcomments.\n",
    "\n",
    "    Returns:\n",
    "        List of dictionaries containing the extracted comment data.\n",
    "    \"\"\"\n",
    "    comments = (\n",
    "        page_soup.find(\"div\", class_=\"forum-container\")\n",
    "        .find(\"div\", id=\"note_children\")\n",
    "        .find_all(\"div\", class_=\"note_with_children\")\n",
    "    )\n",
    "\n",
    "    if in_subcomments:\n",
    "        comments = comments[1:]\n",
    "\n",
    "    comment_data = []\n",
    "    for soup in comments:\n",
    "        if soup.find(\"div\", class_=\"meta_row\").text.strip() != \"[Deleted]\":\n",
    "            title = soup.find(\"h2\", class_=\"note_content_title\").text.strip()\n",
    "            subtitle = soup.find(\"div\", class_=\"meta_row\").span.text.strip()\n",
    "            date_submitted, date_modified = None, None\n",
    "            try:\n",
    "                date_text = soup.find(\"span\", class_=\"date\").text.strip()\n",
    "                dates = re.findall(r\"(\\d{2} \\w{3} \\d{4})\", date_text)\n",
    "                if len(dates) == 2:\n",
    "                    date_submitted, date_modified = dates\n",
    "                elif len(dates) == 1:\n",
    "                    date_submitted = dates[0]\n",
    "            except:\n",
    "                print(f\"Date not found for comment {title} on paper {paper_id}\")\n",
    "            review_type = (\n",
    "                soup.find(\"span\", class_=\"date\").find_next_sibling(\"span\").text.strip()\n",
    "            )\n",
    "\n",
    "            # Extracting all note_contents (i.e., comments) with names and content\n",
    "            note_contents = soup.find_all(\"div\", class_=\"note_contents\")\n",
    "            content_dict = {}\n",
    "            for content in note_contents:\n",
    "                field_name = content.find(\n",
    "                    \"span\", class_=\"note_content_field\"\n",
    "                ).text.strip(\": \")\n",
    "                field_value = content.find(\n",
    "                    \"span\", class_=\"note_content_value\"\n",
    "                ).text.strip()\n",
    "                content_dict[field_name] = field_value\n",
    "\n",
    "            extracted_info = {\n",
    "                \"Title\": title,\n",
    "                \"Authors\": subtitle,\n",
    "                \"Date Submitted\": date_submitted,\n",
    "                \"Date Modified\": date_modified,\n",
    "                \"Review Type\": review_type,\n",
    "                **content_dict,\n",
    "            }\n",
    "            comment_data.append(extracted_info)\n",
    "\n",
    "        try:\n",
    "            view_more_replies_id = soup.select_one(\n",
    "                \":scope > .view-more-replies-container a\"\n",
    "            ).get(\"data-note-id\")\n",
    "            view_more_replies = driver.find_elements(\n",
    "                By.CSS_SELECTOR, \"a[data-note-id='\" + view_more_replies_id + \"']\"\n",
    "            )\n",
    "            if view_more_replies:\n",
    "                start_time = time.time()\n",
    "                driver.execute_script(\"arguments[0].click();\", view_more_replies[0])\n",
    "                end_time = time.time()\n",
    "                interval = end_time - start_time\n",
    "                if interval < TIME:\n",
    "                    time.sleep(TIME - interval)\n",
    "                page_soup = get_page_source(driver)\n",
    "                comment_data = get_comment_data(page_soup, driver, paper_id, True)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if in_subcomments:  # Click back to the parent comment\n",
    "        button = driver.find_element(\n",
    "            By.CSS_SELECTOR, \".forum-container .view-all-replies-container button\"\n",
    "        )\n",
    "        start_time = time.time()\n",
    "        driver.execute_script(\"arguments[0].click();\", button)\n",
    "        end_time = time.time()\n",
    "        interval = end_time - start_time\n",
    "        if interval < TIME:\n",
    "            time.sleep(TIME - interval)\n",
    "        page_soup = get_page_source(driver)\n",
    "\n",
    "    return comment_data\n",
    "\n",
    "\n",
    "def txt_to_list(filename):\n",
    "    \"\"\"\n",
    "    Reads a text file and returns its contents as a list of strings.\n",
    "    Args:\n",
    "    filename: Name of the text file to read.\n",
    "\n",
    "    Returns:\n",
    "        List of strings representing the lines in the text file.\n",
    "    \"\"\"\n",
    "    with open(filename) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "    return content\n",
    "\n",
    "\n",
    "def list_to_txt(filename, id_list):\n",
    "    \"\"\"\n",
    "    Writes a list of IDs to a text file.\n",
    "    Args:\n",
    "    filename: Name of the text file to write.\n",
    "    id_list: List of IDs to write to the file.\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        for item in id_list:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "\n",
    "\n",
    "def format_df(dicts):\n",
    "    \"\"\"\n",
    "    Formats the scraped data into a pandas DataFrame.\n",
    "    Args:\n",
    "    dicts: List of dictionaries containing the scraped data.\n",
    "\n",
    "    Returns:\n",
    "        Formatted pandas DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.DataFrame(dicts)\n",
    "    author_columns = [col for col in df.columns if col.startswith(\"Author \")]\n",
    "    author_names = []\n",
    "    author_ids = []\n",
    "    num_authors = len(author_columns) // 2\n",
    "    for _, row in df.iterrows():\n",
    "        names = [\n",
    "            row[f\"Author {i+1} name\"]\n",
    "            for i in range(num_authors)\n",
    "            if f\"Author {i+1} name\" in row\n",
    "        ]\n",
    "        emails = [\n",
    "            row[f\"Author {i+1} id\"]\n",
    "            for i in range(num_authors)\n",
    "            if f\"Author {i+1} id\" in row\n",
    "        ]\n",
    "        author_names.append(names)\n",
    "        author_ids.append(emails)\n",
    "    df[\"Author Names\"] = author_names\n",
    "    df[\"Author IDs\"] = author_ids\n",
    "    for i in range(1, num_authors + 1):\n",
    "        df.drop(\n",
    "            [f\"Author {i} name\", f\"Author {i} id\"],\n",
    "            axis=1,\n",
    "            inplace=True,\n",
    "            errors=\"ignore\",\n",
    "        )\n",
    "    return df\n",
    "\n",
    "\n",
    "def scrape_page(paper_id):\n",
    "    \"\"\"\n",
    "    Scrapes paper data and comments from a specific paper page.\n",
    "\n",
    "    Args:\n",
    "        paper_id: ID of the paper to scrape.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing the scraped paper data and comments.\n",
    "    \"\"\"\n",
    "    url = \"https://openreview.net/forum?id=\" + paper_id\n",
    "    driver.get(url)\n",
    "    page_soup = get_page_source(driver)\n",
    "    paper_data = get_paper_data(page_soup, paper_id)\n",
    "    comments = get_comment_data(page_soup, driver, paper_id)\n",
    "    paper_data[\"Comments\"] = comments\n",
    "    paper_data[\"Number of comments\"] = len(comments)\n",
    "    return paper_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Safari()\n",
    "TIME = 3.5\n",
    "\n",
    "for year in range(2017, 2024):\n",
    "    id_list = txt_to_list(\"data/id_data/\" + str(year) + \".txt\")\n",
    "    print(\"Number of papers to scrape:\", len(id_list))\n",
    "    print(\"Scraping year\", year)\n",
    "    dicts = []\n",
    "    batch_count = 1\n",
    "    for i, paper_id in enumerate(tqdm(id_list), start=1):\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            dicts.append(scrape_page(paper_id))\n",
    "            end_time = time.time()\n",
    "            if end_time - start_time < TIME:\n",
    "                time.sleep(TIME - (end_time - start_time))\n",
    "\n",
    "            if i % 50 == 0 or i == len(id_list):\n",
    "                df = format_df(dicts)\n",
    "                df.to_csv(\n",
    "                    f\"data/raw_peer_review_data/{year}_batch_{batch_count}.csv\",\n",
    "                    index=False,\n",
    "                )\n",
    "                dicts = []\n",
    "                batch_count += 1\n",
    "        except:\n",
    "            print(\"Error occurred for paper\", paper_id)\n",
    "            time.sleep(TIME)\n",
    "            continue\n",
    "    print(\"Finished scraping year\", year)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
