{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subfield Classification of Research Papers using Text Clustering\n",
    "\n",
    "This script performs text clustering on a dataset of research papers. The main steps include:\n",
    "\n",
    "1. Data Loading and Preprocessing: The script reads peer-reviewed paper data along with associated IDs and publication years. It preprocesses the text data by combining abstracts and keywords, cleaning the text, and filtering out unwanted entries.\n",
    "\n",
    "2. Clustering with TF-IDF and KMeans: Using the TF-IDF representation of the text data, the script applies KMeans clustering to group similar papers. It also generates descriptive cluster names based on the top keywords in each cluster.\n",
    "\n",
    "3. Clustering with Sentence Embeddings and KMeans: The script uses a pre-trained SentenceTransformer model to generate embeddings for the text data and applies KMeans clustering to form clusters based on semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Data Loading and Preprocessing\n",
    "# --------------------------------------\n",
    "\n",
    "# Load the peer review data from a CSV file\n",
    "pr = pd.read_csv(\"data/cleaned_peer_review_data_brief/cleaned_peer_review_data_brief.csv\")\n",
    "\n",
    "# Load the paper IDs for each year and create a mapping of paper_id to year\n",
    "ids = {}\n",
    "for year in range(2017, 2024):\n",
    "    with open(f\"data/id_data/{year}.txt\") as f:\n",
    "        ids[year] = f.read().splitlines()\n",
    "\n",
    "# Invert the dictionary to map paper_id to year\n",
    "ids = {paper_id: year for year, paper_ids in ids.items() for paper_id in paper_ids}\n",
    "\n",
    "# Map the paper_id to year and add as a new column\n",
    "pr[\"year\"] = pr[\"paper_id\"].map(ids)\n",
    "\n",
    "# Filter out unwanted entries and duplicates\n",
    "pr = pr[~pr[\"paper_name\"].isin([\"paper_name\", \"Withdraw\"])]\n",
    "pr = pr.drop_duplicates(subset=[\"paper_id\", \"year\"])\n",
    "\n",
    "# Select relevant columns and filter out entries with missing abstracts\n",
    "df = pr[[\"paper_name\", \"abstract\", \"keywords\", \"paper_id\"]]\n",
    "df = df[df[\"abstract\"].notna()]\n",
    "\n",
    "# Fill missing keywords with empty strings and combine abstract and keywords into one text field\n",
    "df[\"keywords\"] = df[\"keywords\"].fillna(\"\")\n",
    "df['text'] = df['abstract'] + ' ' + df['keywords']\n",
    "\n",
    "# Clean the text by removing certain characters\n",
    "df['text'] = df['text'].str.replace('\"', '').replace(\"'\", '').replace(\"\\n\", \" \")\n",
    "\n",
    "# Reset the index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Step 2: Clustering with TF-IDF vectors and KMeans\n",
    "# -------------------------------------------------\n",
    "\n",
    "# Vectorize the text data using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "# Specify the number of clusters\n",
    "k_tfidf = 10\n",
    "\n",
    "# Perform KMeans clustering\n",
    "kmeans_tfidf = KMeans(n_clusters=k_tfidf, random_state=42)\n",
    "kmeans_tfidf.fit(X_tfidf)\n",
    "\n",
    "# Assign cluster labels to the DataFrame\n",
    "df['cluster_tfidf'] = kmeans_tfidf.labels_\n",
    "\n",
    "# Generate cluster names based on top TF-IDF features in each cluster\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "cluster_names = {}\n",
    "top_n = 3  # Number of top keywords to consider for cluster names\n",
    "\n",
    "for i in range(k_tfidf):\n",
    "    # Get indices of papers in the current cluster\n",
    "    cluster_indices = np.where(kmeans_tfidf.labels_ == i)[0]\n",
    "    \n",
    "    # Sum TF-IDF scores for each feature within the cluster\n",
    "    cluster_sum = X_tfidf[cluster_indices].sum(axis=0)\n",
    "    \n",
    "    # Get top feature indices\n",
    "    top_indices = np.argsort(cluster_sum).tolist()[0][-top_n:][::-1]\n",
    "    \n",
    "    # Get the corresponding feature names\n",
    "    top_keywords = [feature_names[idx] for idx in top_indices]\n",
    "    \n",
    "    # Create a cluster name\n",
    "    cluster_name = '_'.join(top_keywords)\n",
    "    cluster_names[i] = cluster_name\n",
    "\n",
    "# Print the cluster names\n",
    "for i in range(k_tfidf):\n",
    "    print(f\"Cluster {i+1} ({cluster_names[i]}):\\n\")\n",
    "    # Optionally, print paper names or other details\n",
    "    # cluster_papers = df[df['cluster_tfidf'] == i]\n",
    "    # print(cluster_papers['paper_name'].tolist())\n",
    "\n",
    "# Step 3: Clustering with Sentence Embeddings and KMeans\n",
    "# ------------------------------------------------------\n",
    "\n",
    "# Load a pre-trained SentenceTransformer model\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "\n",
    "# Generate embeddings for the text data\n",
    "embeddings = model.encode(df['text'].tolist())\n",
    "\n",
    "# Specify the number of clusters\n",
    "k_embeddings = 3\n",
    "\n",
    "# Perform KMeans clustering on the embeddings\n",
    "kmeans_embeddings = KMeans(n_clusters=k_embeddings, random_state=42)\n",
    "kmeans_embeddings.fit(embeddings)\n",
    "\n",
    "# Assign cluster labels to the DataFrame\n",
    "df['cluster_embeddings'] = kmeans_embeddings.labels_\n",
    "\n",
    "# Optionally, print cluster assignments or analyze clusters\n",
    "for i in range(k_embeddings):\n",
    "    print(f\"Embedding Cluster {i+1}:\\n\")\n",
    "    # cluster_papers = df[df['cluster_embeddings'] == i]\n",
    "    # print(cluster_papers['paper_name'].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Classification of Fields through Keywords\n",
    "\n",
    "Simply classify papers based on the keywords in the abstracts and titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tran_et_al = {\n",
    "    'theory': ['theorem', 'prove', 'proof'],\n",
    "    'cv': ['vision', 'object detection', 'segmentation', 'pose estimation', 'optical character recognition', 'ocr', 'structure from motion', 'recognition', 'cnn', 'convolution', 'vision'],\n",
    "    'nlp': ['language', 'nlp', 'named-entity', 'translation', 'translate', 'word embeddings', 'speech', 'bert', 'transformer', 'elmo', 'attention'],\n",
    "    'robustness': ['adversarial', 'attack', 'poison', 'backdoor','robust'],\n",
    "    'generative': ['generative', 'gan', 'vae', 'autoencoder', 'auto-encoder', 'diffusion', 'generation'],\n",
    "    'optimization': ['optimization', 'convergence', 'convex', 'stationary point'],\n",
    "    'graphs': ['graph', 'gnn', 'node', 'edge'],\n",
    "    'bayesian': ['bayes', 'prior', 'posterior', 'gmm', 'gaussian mixture', 'mixture model', 'mcmc', 'monte carlo'],\n",
    "}\n",
    "\n",
    "df[\"total\"] = 0\n",
    "\n",
    "for col in tran_et_al.keys():\n",
    "    df[col] = 0\n",
    "    for word in tran_et_al[col]:\n",
    "        df[col] += df[\"text\"].str.contains(word).astype(int)\n",
    "    df[col] = (df[col] > 0) * 1\n",
    "    df[\"total\"] += df[col]\n",
    "\n",
    "for col in tran_et_al.keys():\n",
    "    df[col] = df[col] / df[\"total\"]\n",
    "    df[col] = df[col].fillna(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peer_review",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
